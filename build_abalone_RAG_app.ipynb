{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3733e10f",
   "metadata": {},
   "source": [
    "# Abalone RAG\n",
    "\n",
    "This notebook describes the development of the RAG application that is featured on [HuggingFace Spaces](https://huggingface.co/spaces/LoneWolfgang/Abalone-RAG-Demo).\n",
    "\n",
    "This notebook is structured into three parts.\n",
    "  1. `Build a Corpus`: Explains how text and pdf files may be converted into a structured corpus for indexing and retrieval.\n",
    "  2. `Preprocess Paragraphs for RAG`: Goes deeper into preprocessing, explaining how text is cleaned and segmented for an optimal selection of context.\n",
    "  3. `Indexing and Retrieval`: Demonstrates how to create a FAISS index using SBERT. Also includes reranking using Cross Encoders.\n",
    "  4. `RAG with TinyLlama`: Ties everything together into a RAG system. Fine tune a TinyLlama to improve generation quality.\n",
    "\n",
    "\n",
    "Start by installing dependancies. Its reccomended that you run this within a virtual environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eb26e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install -r \"requirements.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b600c82f",
   "metadata": {},
   "source": [
    "## Build a Corpus\n",
    "\n",
    "A corpus is a large, diverse collection of texts used for analysis and modelling. Modern corpora include billions or trillions of tokens. For demonstration, our toy corpus will only include 10 texts. \n",
    "\n",
    "We start by extracting text from its raw form, .txt or .pdf, and placing the contents in a pandas dataframe.\n",
    "\n",
    "The `Loader` will only handle .pdf and .txt documents:  \n",
    "  - For **pdfs**, it uses `PyMuPDF` (fitz). Text blocks are extracted and treated as paragraphs.\n",
    "  - For **txt**, we make the heuristic assumption that complete paragraphs are seperated by newlines.\n",
    "\n",
    "Let's start by tabulating our text data. At the end, we save the data as .parquet for easy access later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85010fc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building dataframe from documents/pdf/Jenkins2000.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3dfa162d5c8f442fa2819b2d97dc7ce1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting text blocks:   0%|          | 0/160 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted document to documents/parquet/Jenkins2000.parquet\n",
      "Building dataframe from documents/pdf/leighton2008.pdf\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc0f428135843f3b671fc9dbfd4c927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting text blocks:   0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved extracted document to documents/parquet/leighton2008.parquet\n",
      "Saved extracted document to documents/parquet/fisheriesnoaa.parquet\n",
      "Saved extracted document to documents/parquet/marinebio.parquet\n",
      "Saved extracted document to documents/parquet/sushiuniversity.parquet\n",
      "Saved extracted document to documents/parquet/tokyofoundation.parquet\n",
      "Saved extracted document to documents/parquet/visitcalifornia.parquet\n",
      "Saved extracted document to documents/parquet/wikipedia.parquet\n",
      "Saved extracted document to documents/parquet/xerces.parquet\n"
     ]
    }
   ],
   "source": [
    "from modules import (\n",
    "    get_files,\n",
    "    Loader\n",
    ")\n",
    "from pathlib import Path\n",
    "\n",
    "# Select the directory where documents are saved.\n",
    "DOCS = Path(\"documents\")\n",
    "\n",
    "# Get files retrieves all files from the input directory, optionally filtering for a specefic extention.\n",
    "files = get_files(DOCS, extension=\"pdf\") + get_files(DOCS, extension=\"txt\")\n",
    "\n",
    "# For each file, load it into a dataframe and save it to a parquet file.\n",
    "for file in files:\n",
    "    loader = Loader(file)\n",
    "    loader.load()\n",
    "    loader.save_as_parquet(DOCS / \"parquet\" / f\"{file.stem}.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5954ab",
   "metadata": {},
   "source": [
    "Now that the raw text has been converted to dataframes and saved to .parquet, it can be accessed much faster.\n",
    "\n",
    "This time, lets load all of our parqet files and combine them using the `Corpus` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fcec82e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>paragraph</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>Volume 31, Number 1, January 2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>¥ SpecJat issue:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>'-&gt;?,tli Genetic Issues in Aquaculture Guest e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>Blackwell Science</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>Aquaculture Research</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>xerces</td>\n",
       "      <td>2. Protection of populations and their habitat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2703</th>\n",
       "      <td>xerces</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2704</th>\n",
       "      <td>xerces</td>\n",
       "      <td>3. Captive propagation for enhancement of wild...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2705</th>\n",
       "      <td>xerces</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2706</th>\n",
       "      <td>xerces</td>\n",
       "      <td>4. The development of a plan for public outrea...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2707 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_id                                          paragraph\n",
       "0     Jenkins2000                  Volume 31, Number 1, January 2000\n",
       "1     Jenkins2000                                   ¥ SpecJat issue:\n",
       "2     Jenkins2000  '->?,tli Genetic Issues in Aquaculture Guest e...\n",
       "3     Jenkins2000                                  Blackwell Science\n",
       "4     Jenkins2000                               Aquaculture Research\n",
       "...           ...                                                ...\n",
       "2702       xerces  2. Protection of populations and their habitat...\n",
       "2703       xerces                                                   \n",
       "2704       xerces  3. Captive propagation for enhancement of wild...\n",
       "2705       xerces                                                   \n",
       "2706       xerces  4. The development of a plan for public outrea...\n",
       "\n",
       "[2707 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import Corpus\n",
    "\n",
    "# This time, build a list of parquet files.\n",
    "files = get_files(DOCS, \"parquet\")\n",
    "\n",
    "# Place the files into a generator\n",
    "loaders = (Loader(file) for file in files)\n",
    "\n",
    "# Feed them into the Corpus object. It will combine the paragraphs into a single dataframe\n",
    "corpus = Corpus(*loaders)\n",
    "\n",
    "corpus.paragraphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9cf4770",
   "metadata": {},
   "source": [
    "Looking at the paragraphs extracted from the corpus, several issues are immediately apparent: many paragraphs are underlength, and some content is missing entirely. These problems are addressed by the next component: the `Preprocessor`.\n",
    "\n",
    "## Preprocess paragraphs for RAG\n",
    "\n",
    "The `Preprocessor` object performs the following functions:\n",
    "\n",
    "  1. **Filter underlength paragraphs:**  \n",
    "  Underlength paragraphs are heuristically removed, eliminating most unwanted material such as section headings, captions, references, and miscellaneous floating text.  \n",
    "\n",
    "  2. **Split paragraphs into segments:**  \n",
    "  Remaining paragraphs are split into segments with clean sentence boundaries and token lengths optimized for the embedding model’s context window.\n",
    "\n",
    "  3. **Split segments into sentences:**  \n",
    "  Each segment is further divided into individual sentences, which are used downstream for sentence-level highlighting.\n",
    "  4. **Compute and index embeddings:**  \n",
    "  Segment embeddings are computed and indexed using FAISS for efficient retrieval.\n",
    "\n",
    "The `Preprocessor` consists of four subobjects:\n",
    "  - `Corpus`: Which we composed in the previous step. Serves a datafrome with the columns `document_id` and `paragraph`\n",
    "  - `SentenceEmbedder`: Uses a Sentence Transformer (SBERT) to compute embeddings.\n",
    "  - `SentenceExtractor`: Uses a spaCy model to mark clean sentence boundaries.\n",
    "  - `Segmenter`: Uses the tokenizer for SBERT to mark optimzed segment boundaries for the context window of the SentenceEmbedder.\n",
    "\n",
    "The next few cells where demonstrate the functionality of each object. For a deeper technical understanding, please look at the code in the file `modules.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8893c795",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Start by running this code to initiate our models and a paragraph for demonstration.\n",
    "\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import spacy\n",
    "\n",
    "SBERT = SentenceTransformer(\"all-MiniLM-L12-v2\")\n",
    "TOKENIZER = SBERT.tokenizer\n",
    "SPACY = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "paragraph = r\"\"\"\n",
    "Tasmania supplies about 25% of the yearly world abalone harvest.[41] Around 12,500 Tasmanians recreationally fish for blacklip and greenlip abalone. For blacklip abalone, the size limit varies between 138 mm (5.4 in) for the southern end of the state and 127 mm (5.0 in) for the northern end of the state.[42] Greenlip abalone have a minimum size of 145 mm (5.7 in), except for an area around Perkins Bay in the north of the state where the minimum size is 132 millimetres (5.2 in). With a recreational abalone licence, the bag limit is 10 per day, with a total possession limit of 20. Scuba diving for abalone is allowed, and has a rich history in Australia. (Scuba diving for abalone in the states of New South Wales and Western Australia is illegal; a free-diving catch limit of two is allowed).[43][44]Victoria has had an active abalone fishery since the late 1950s. The state is sectioned into three fishing zones, Eastern, Central and Western, with each fisher required a zone-allocated licence. Harvesting is performed by divers using surface-supplied air \"hookah\" systems operating from runabout-style, outboard-powered boats. While the diver seeks out colonies of abalone amongst the reef beds, the deckhand operates the boat, known as working \"live\" and stays above where the diver is working. Bags of abalone pried from the rocks are brought to the surface by the diver or by way of \"shot line\", where the deckhand drops a weighted rope for the catch bag to be connected then retrieved. Divers measure each abalone before removing from the reef and the deckhand remeasures each abalone and removes excess weed growth from the shell. Since 2002, the Victorian industry has seen a significant decline in catches, with the total allowable catch reduced from 1440 to 787 tonnes for the 2011/12 fishing year, due to dwindling stocks and most notably the abalone virus ganglioneuritis, which is fast-spreading and lethal to abalone stocks. Sport harvesting of red abalone is permitted with a California fishing license and an abalone stamp card. In 2008, the abalone card also came with a set of 24 tags. This was reduced to 18 abalone per year in 2014, and as of 2017 the limit has been reduced to 12, only nine of which may be taken south of Mendocino County. Legal-size abalone must be tagged immediately.[45] Abalone may only be taken using breath-hold techniques or shorepicking; scuba diving for abalone is strictly prohibited.[46] Taking of abalone is not permitted south of the mouth of San Francisco Bay.[47] A size minimum of 7 in (180 mm) measured across the shell is in place. A person may be in possession of only three abalone at any given time.[48][49]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3262d968",
   "metadata": {},
   "source": [
    "The `SentenceExtractor` will pick out the string position of the first and last character of each sentence in the paragraph. You can use these to extract cleane sentences from the paragraph:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6054b8b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  sentence_boundaries                                           sentence\n",
      "0        (0, (0, 69))  \\nTasmania supplies about 25% of the yearly wo...\n",
      "1      (1, (70, 149))  Around 12,500 Tasmanians recreationally fish f...\n",
      "2     (2, (150, 310))  For blacklip abalone, the size limit varies be...\n",
      "3     (3, (311, 483))  Greenlip abalone have a minimum size of 145 mm...\n",
      "4     (4, (484, 586))  With a recreational abalone licence, the bag l...\n",
      "\n",
      "Sample Sentence:\n",
      "\n",
      "Scuba diving for abalone is allowed, and has a rich history in Australia.\n"
     ]
    }
   ],
   "source": [
    "from modules import SentenceExtractor\n",
    "import pandas as pd\n",
    "\n",
    "sentence_extractor = SentenceExtractor(SPACY)\n",
    "\n",
    "sentence_boundaries = sentence_extractor.sentence_boundaries(paragraph)\n",
    "sentences = [paragraph[b[1][0]: b[1][1]] for b in sentence_boundaries]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"sentence_boundaries\": sentence_boundaries,\n",
    "    \"sentence\": sentences\n",
    "})\n",
    "\n",
    "print()\n",
    "print(df.head())\n",
    "print()\n",
    "print(\"Sample Sentence:\")\n",
    "print()\n",
    "print(df.loc[5, \"sentence\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c021cfa5",
   "metadata": {},
   "source": [
    "The `Segmenter` will pick out the string positions to extract segments that will fill the context window of the embedding model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6477faaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  segment_boundaries                                            segment\n",
      "0           (1, 488)  Tasmania supplies about 25% of the yearly worl...\n",
      "1         (361, 899)  7 in), except for an area around Perkins Bay i...\n",
      "2        (774, 1341)  limit of two is allowed).[43][44]Victoria has ...\n",
      "3       (1203, 1789)  , the deckhand operates the boat, known as wor...\n",
      "4       (1634, 2201)  the shell. Since 2002, the Victorian industry ...\n",
      "\n",
      "Sample Segment:\n",
      "\n",
      "limit of two is allowed).[43][44]Victoria has had an active abalone fishery since the late 1950s. The state is sectioned into three fishing zones, Eastern, Central and Western, with each fisher required a zone-allocated licence. Harvesting is performed by divers using surface-supplied air \"hookah\" systems operating from runabout-style, outboard-powered boats. While the diver seeks out colonies of abalone amongst the reef beds, the deckhand operates the boat, known as working \"live\" and stays above where the diver is working. Bags of abalone pried from the rocks\n"
     ]
    }
   ],
   "source": [
    "from modules import Segmenter\n",
    "import pandas as pd\n",
    "\n",
    "segmenter = Segmenter(TOKENIZER)\n",
    "\n",
    "segment_boundaries = segmenter.segment_boundaries(paragraph)\n",
    "segments = [paragraph[b[0]: b[1]] for b in segment_boundaries]\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"segment_boundaries\": segment_boundaries,\n",
    "    \"segment\": segments\n",
    "})\n",
    "\n",
    "print()\n",
    "print(df.head())\n",
    "print()\n",
    "print(\"Sample Segment:\")\n",
    "print()\n",
    "print(df.loc[2, \"segment\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96c9eeae",
   "metadata": {},
   "source": [
    "Finally, the `Embedder` will take texts and return embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88ac56a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  segment_boundaries                                            segment  \\\n",
      "0           (1, 488)  Tasmania supplies about 25% of the yearly worl...   \n",
      "1         (361, 899)  7 in), except for an area around Perkins Bay i...   \n",
      "2        (774, 1341)  limit of two is allowed).[43][44]Victoria has ...   \n",
      "3       (1203, 1789)  , the deckhand operates the boat, known as wor...   \n",
      "4       (1634, 2201)  the shell. Since 2002, the Victorian industry ...   \n",
      "\n",
      "                                          embeddings  \n",
      "0  [0.07767071, -0.0057242527, -0.05175696, 0.008...  \n",
      "1  [0.073290914, -0.0001915124, -0.012798946, 0.0...  \n",
      "2  [0.05552063, -0.01772244, 0.00100956, 0.013807...  \n",
      "3  [0.05004228, 0.03330825, -0.016384125, 0.03617...  \n",
      "4  [0.031442273, -0.0028620476, -0.041476786, -0....  \n"
     ]
    }
   ],
   "source": [
    "from modules import SentenceEmbedder\n",
    "\n",
    "embedder = SentenceEmbedder(SBERT)\n",
    "embeddings = embedder.embed(df.segment.tolist())\n",
    "\n",
    "df[\"embeddings\"] = list(embeddings)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b45d68",
   "metadata": {},
   "source": [
    "The `Preprocessor` object ties all of these functions together. Using the sentence and segment boundaries, it infers the segment boundaries with clean sentence edges and optimal token length. It performs other cleaning functions as well, such as filtering underlength paragraphs and deduplicating segments with common anchors.\n",
    "\n",
    "This time, instead of the pargarph, we will preprocess the full corpus that we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e33fd79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f63e337e564a48ed8968bc703c0ce6df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sentence boundaries:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ccb3a161988b45db840953e8565db9db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting sentences:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c02cb3dd72254b8a8668d60e9e3285a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Segment boundaries:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dbcd49cf9fd46c1882c83602164b3a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Aligning segments:   0%|          | 0/949 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9e6c888dd8149e189b25a07bec6fd98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Embedding segments:   0%|          | 0/1525 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>document_id</th>\n",
       "      <th>sentences</th>\n",
       "      <th>segment</th>\n",
       "      <th>segment_embedding</th>\n",
       "      <th>segment_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>[Editorial Board G. Allan Taylors Beach, Austr...</td>\n",
       "      <td>Editorial Board G. Allan Taylors Beach, Austra...</td>\n",
       "      <td>[0.025996055, 0.018459106, 0.01713328, 0.02857...</td>\n",
       "      <td>Jenkins2000-000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>[S.J. Kaushik Saint-Pie-sur-Nivelle, France G....</td>\n",
       "      <td>S.J. Kaushik Saint-Pie-sur-Nivelle, France G.W...</td>\n",
       "      <td>[0.05069863, 0.09893627, 0.050703455, -0.05172...</td>\n",
       "      <td>Jenkins2000-001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>[Aims and scope Aquaculture Research is an int...</td>\n",
       "      <td>Aims and scope Aquaculture Research is an inte...</td>\n",
       "      <td>[-0.010691009, 0.063925244, -0.05688352, -0.04...</td>\n",
       "      <td>Jenkins2000-002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>[In addition to publishing papers reporting re...</td>\n",
       "      <td>In addition to publishing papers reporting res...</td>\n",
       "      <td>[0.057243183, 0.07570791, -0.0029028028, 0.031...</td>\n",
       "      <td>Jenkins2000-003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jenkins2000</td>\n",
       "      <td>[Despatch Aquaculture Research is despatched w...</td>\n",
       "      <td>Despatch Aquaculture Research is despatched wi...</td>\n",
       "      <td>[0.0126762865, -0.020529907, -0.003612882, -0....</td>\n",
       "      <td>Jenkins2000-004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1520</th>\n",
       "      <td>xerces</td>\n",
       "      <td>[By the 1990s the species was nearly extirpate...</td>\n",
       "      <td>By the 1990s the species was nearly extirpated...</td>\n",
       "      <td>[0.1002283, 0.034409385, -0.08829971, 0.012946...</td>\n",
       "      <td>xerces-007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1521</th>\n",
       "      <td>xerces</td>\n",
       "      <td>[Approximately 1,600 individuals remain, and i...</td>\n",
       "      <td>Approximately 1,600 individuals remain, and it...</td>\n",
       "      <td>[0.04162371, 0.005460795, 0.009169072, 0.02119...</td>\n",
       "      <td>xerces-008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>xerces</td>\n",
       "      <td>[The white abalone was listed as an endangered...</td>\n",
       "      <td>The white abalone was listed as an endangered ...</td>\n",
       "      <td>[0.029424477, 0.013186502, -0.041589364, 0.067...</td>\n",
       "      <td>xerces-009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1523</th>\n",
       "      <td>xerces</td>\n",
       "      <td>[The main threats to the continued existence o...</td>\n",
       "      <td>The main threats to the continued existence of...</td>\n",
       "      <td>[0.0076671536, 0.07647452, -0.07140951, 0.0767...</td>\n",
       "      <td>xerces-010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1524</th>\n",
       "      <td>xerces</td>\n",
       "      <td>[Other threats include inadequate enforcement ...</td>\n",
       "      <td>Other threats include inadequate enforcement o...</td>\n",
       "      <td>[0.056884754, 0.041740835, 0.006424177, 0.0476...</td>\n",
       "      <td>xerces-011</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1525 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      document_id                                          sentences  \\\n",
       "0     Jenkins2000  [Editorial Board G. Allan Taylors Beach, Austr...   \n",
       "1     Jenkins2000  [S.J. Kaushik Saint-Pie-sur-Nivelle, France G....   \n",
       "2     Jenkins2000  [Aims and scope Aquaculture Research is an int...   \n",
       "3     Jenkins2000  [In addition to publishing papers reporting re...   \n",
       "4     Jenkins2000  [Despatch Aquaculture Research is despatched w...   \n",
       "...           ...                                                ...   \n",
       "1520       xerces  [By the 1990s the species was nearly extirpate...   \n",
       "1521       xerces  [Approximately 1,600 individuals remain, and i...   \n",
       "1522       xerces  [The white abalone was listed as an endangered...   \n",
       "1523       xerces  [The main threats to the continued existence o...   \n",
       "1524       xerces  [Other threats include inadequate enforcement ...   \n",
       "\n",
       "                                                segment  \\\n",
       "0     Editorial Board G. Allan Taylors Beach, Austra...   \n",
       "1     S.J. Kaushik Saint-Pie-sur-Nivelle, France G.W...   \n",
       "2     Aims and scope Aquaculture Research is an inte...   \n",
       "3     In addition to publishing papers reporting res...   \n",
       "4     Despatch Aquaculture Research is despatched wi...   \n",
       "...                                                 ...   \n",
       "1520  By the 1990s the species was nearly extirpated...   \n",
       "1521  Approximately 1,600 individuals remain, and it...   \n",
       "1522  The white abalone was listed as an endangered ...   \n",
       "1523  The main threats to the continued existence of...   \n",
       "1524  Other threats include inadequate enforcement o...   \n",
       "\n",
       "                                      segment_embedding       segment_id  \n",
       "0     [0.025996055, 0.018459106, 0.01713328, 0.02857...  Jenkins2000-000  \n",
       "1     [0.05069863, 0.09893627, 0.050703455, -0.05172...  Jenkins2000-001  \n",
       "2     [-0.010691009, 0.063925244, -0.05688352, -0.04...  Jenkins2000-002  \n",
       "3     [0.057243183, 0.07570791, -0.0029028028, 0.031...  Jenkins2000-003  \n",
       "4     [0.0126762865, -0.020529907, -0.003612882, -0....  Jenkins2000-004  \n",
       "...                                                 ...              ...  \n",
       "1520  [0.1002283, 0.034409385, -0.08829971, 0.012946...       xerces-007  \n",
       "1521  [0.04162371, 0.005460795, 0.009169072, 0.02119...       xerces-008  \n",
       "1522  [0.029424477, 0.013186502, -0.041589364, 0.067...       xerces-009  \n",
       "1523  [0.0076671536, 0.07647452, -0.07140951, 0.0767...       xerces-010  \n",
       "1524  [0.056884754, 0.041740835, 0.006424177, 0.0476...       xerces-011  \n",
       "\n",
       "[1525 rows x 5 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from modules import Preprocessor\n",
    "\n",
    "preprocessor = Preprocessor(\n",
    "    corpus,\n",
    "    embedder,\n",
    "    sentence_extractor,\n",
    "    segmenter\n",
    ")\n",
    "\n",
    "kwargs = {\n",
    "    # Controls the minimum viable lenght of a paragraph. Defaults to 32.\n",
    "    \"min_paragraph_length\": 32,\n",
    "\n",
    "    # Controls the number of overlapping tokens between segments. Defaults to 32.\n",
    "    \"stride\": 32\n",
    "}\n",
    "\n",
    "preprocessor.preprocess(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edff4e7",
   "metadata": {},
   "source": [
    "## Indexing and Retrieval\n",
    "\n",
    "Next, we build a FAISS index using the `IngestPipeline`. This object takes the preprocessed documents and produces two outputs:\n",
    "\n",
    "- **FAISS index**: An ordered collection of precomputed segment embeddings, optimized for fast similarity search.\n",
    "- **DocStore**: A companion data store containing all relevant metadata for each segment—most importantly, the segment text itself. Given the indices of the top-k retrieved vectors from FAISS, the DocStore is used to look up and return the corresponding text and attributes.\n",
    "\n",
    "Optionally, the `IngestPipeline` can also accept a metadata dictionary keyed by `document_id`. This metadata is joined onto the segments during ingestion. In this example, we use it to attach source URLs, but it is also common to include fields such as document title, author, publication date, or a trust score to support richer and more controllable RAG applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6345d970",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved index to index/index.faiss\n",
      "Saved docstore with 1525 entries to index/docstore.pkl\n",
      "Number of vectors in index: 1525\n",
      "Saved index to index/index.faiss\n",
      "Saved docstore with 1525 entries to index/docstore.pkl\n",
      "Number of vectors in index: 1525\n",
      "✓ Repo exists: LoneWolfgang/Abalone-mini-index\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37b2cbc10474ac6847661657dc5ab61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eefead7eeccb42e9995a781f91709e70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa7cd5eb52e6444185993e5a179ab1aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cfdc4121b7a14f1983e670ed3dbf31d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully pushed index and docstore to Hugging Face Hub\n"
     ]
    }
   ],
   "source": [
    "from modules import IngestPipeline, load_json\n",
    "\n",
    "# Load the metadata containing URLS for the documents\n",
    "METADATA = load_json(\"documents/metadata.json\")\n",
    "\n",
    "INDEX = \"index\"\n",
    "\n",
    "# Initiate the pipeline\n",
    "pipeline = IngestPipeline(\n",
    "    preprocessor,\n",
    "    metadata=METADATA\n",
    ")\n",
    "\n",
    "# Run activates the preprocessor, and builds the FAISS index and DocStore Objects\n",
    "pipeline.run()\n",
    "\n",
    "# This will save your index to a specefied directory.\n",
    "pipeline.save_index(INDEX)\n",
    "\n",
    "# Optionally, push your index to HuggingFace hub.\n",
    "# This is reccomended if you want to use your index within HuggingFace Spaces.\n",
    "# You will need to set up a login mechanism. Please see the HF Docs\n",
    "\n",
    "pipeline.save_index(\n",
    "    INDEX,\n",
    "    push_to_hub=True,\n",
    "    repo_id = \"LoneWolfgang/Abalone-mini-index\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe5cea6",
   "metadata": {},
   "source": [
    "Now that we have built our index, let’s test the `Retriever`.\n",
    "\n",
    "The Retriever combines three components:\n",
    "  - **Index**: Built in the previous step.\n",
    "  - **SBERT (bi-encoder)**: Used to embed queries and retrieve the top-k candidate segments.\n",
    "  - **Cross Encoder**: A more powerful retrieval model, used to select the best matching segment and the best sentence for highlighting.\n",
    "\n",
    "Cross encoders are generally regarded as superior to bi-encoders (e.g., SBERT) for retrieval accuracy. Unlike bi-encoders, which embed queries and documents independently, cross encoders compute a similarity score from a joint encoding of both the query and the candidate text. Because cross encoders require the query at inference time, their embeddings cannot be precomputed. As a result, while they offer higher precision and recall, they do not scale as well as bi-encoders.\n",
    "\n",
    "By using bi-encoders for initial retrieval and cross encoders to re-rank the top-k candidates, we combine the scalability of bi-encoders with the accuracy of cross encoders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff252c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from modules import Retriever\n",
    "from sentence_transformers import CrossEncoder\n",
    "\n",
    "CROSS_ENDCODER = CrossEncoder(\"cross-encoder/ms-marco-MiniLM-L-6-v2\")\n",
    "\n",
    "retriever = Retriever(\n",
    "    index_dir = INDEX,\n",
    "    sbert = SBERT,\n",
    "    cross_encoder = CROSS_ENDCODER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c3522e95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What age do abalone express their gender?\n",
      "\n",
      "Best Segment:\n",
      "**White abalone have a life expectancy of about thirty-five to forty-years, and reach sexual maturity around 4 to 6 years.** They are dioecious, meaning that they have separate sexes (NMFS 2006). They have high fecundity and reproduce through ‘broadcast spawning’; millions of eggs or sperm are released during spawning events (Leighton 2000). In order to successfully reproduce, the male and female abalone must be close enough together – within a few meters (Davis et al. 1996). Commercial fishing pressures reduced its population densities so low that remaining individuals are too scattered to successfully reproduce.\n",
      "\n",
      "Highlighted Sentence:\n",
      "White abalone have a life expectancy of about thirty-five to forty-years, and reach sexual maturity around 4 to 6 years.\n",
      "\n",
      "Source:\n",
      "https://www.xerces.org/endangered-species/species-profiles/at-risk-aquatic-invertebrates/white-abalone\n"
     ]
    }
   ],
   "source": [
    "query = \"What age do abalone express their gender?\"\n",
    "\n",
    "result = retriever.retrieve(query, metadata_fields=[\"url\"])\n",
    "\n",
    "print(\"Query:\")\n",
    "print(query)\n",
    "print()\n",
    "print(\"Best Segment:\")\n",
    "print(result[\"text\"])\n",
    "print()\n",
    "print(\"Highlighted Sentence:\")\n",
    "print(result[\"highlight\"])\n",
    "print()\n",
    "print(\"Source:\")\n",
    "print(result[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb33f645",
   "metadata": {},
   "source": [
    "## RAG with TinyLlama\n",
    "\n",
    "Now that retrieval is all set up, we can put together a RAG application. For this demonstration, we will be using [TinyLlama](https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0). This is a 1.1B parameter model. It's not particularly powerful, nor does it it run well on a CPU. However, it is decently expressive, and its usable. A nice tradeoff for a tech demo.\n",
    "\n",
    "Let's put together an app using the `RAG` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9dc35408",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from modules import RAG\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"TinyLlama/TinyLlama-1.1B-Chat-v1.0\",\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.1\n",
    "    )\n",
    "\n",
    "rag = RAG(retriever, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cebca07c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "How deep in the water do abalone live?\n",
      "\n",
      "Response:\n",
      "Answer: Abalone live at depths of 50 to 180 feet, making them the deepest living abalone species.\n",
      "\n",
      "Context:\n",
      "White abalone live on low-relief rocky substrates, typically alongside sand channels, which tend to accumulate the algae they eat. **They are usually found at depths of 50 to 180 feet, making them the deepest living abalone species.** Historically, white abalone were found in the Pacific Ocean from Point Conception, California, to Punta Abreojos, Baja California, in Mexico. In California, they were most abundant at offshore islands (especially San Clemente and Santa Catalina Islands) and submerged banks (primarily Tanner and Cortes Banks).\n",
      "\n",
      "Source:\n",
      "https://www.fisheries.noaa.gov/species/white-abalone\n"
     ]
    }
   ],
   "source": [
    "query = \"How deep in the water do abalone live?\"\n",
    "\n",
    "result = rag.answer_query(query)\n",
    "\n",
    "print(\"Query:\")\n",
    "print(query)\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(result[\"response\"])\n",
    "print()\n",
    "print(\"Context:\")\n",
    "print(result[\"context\"])\n",
    "print()\n",
    "print(\"Source:\")\n",
    "print(result[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889fcadd",
   "metadata": {},
   "source": [
    "Generally, the responses range in accuracy. The biggest problem that I had was that it tended to be overly verbose. Let's see if we can improve the quality of responses through finetuning.\n",
    "\n",
    "A finetuning dataset using GPT-5\n",
    "  - First, ChatGPT-5 was instructed to generate 100 queries about abalone covering a range of topics.\n",
    "  - Then, provided with the queries and retrieval results, it was instructed to generate a concise and accurate response.\n",
    "\n",
    "Let's look at the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4cde677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What diseases are common in abalone aquaculture?\n",
      "\n",
      "Context:\n",
      "An infestation of sabellids is generally not a life-threatening situation for the abalone.  **It is, however, a very serious problem for an aquaculture producer. ** If the abalone become heavily infested, their growth rate plummets.  In an aquaculture environment, reduced growth rates mean added costs and reduced profits. In addition, the appearance of the affected shells limits the product value in the premium markets for live abalone.  Due to the shape of their shells, heavily infested abalone are often unable to right\n",
      "\n",
      "Response:\n",
      "Sabellid infestations are the main disease concern, reducing growth and shell quality as noted in the context.\n",
      "\n",
      "\n",
      "Query:\n",
      "How do abalones protect themselves from desiccation at low tide?\n",
      "\n",
      "Context:\n",
      "themselves when dislodged from their substrate. The mass handlings associated with aquaculture production inevitably lead to higher mortalities when working with infested abalone, as it is impossible to ensure that each abalone is placed right side up in a tank. Often, an infestation of the sabellids will result in a loss of the gill pores. **This leaves the abalone more susceptible to mortalities caused by system malfunctions or chronically poor conditions in the growing tanks. **\n",
      "\n",
      "Response:\n",
      "The context does not describe desiccation behavior; it focuses on loss of gill pores increasing vulnerability in tanks.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train = load_dataset(\"LoneWolfgang/finetune-chat-for-abalone-RAG\")['train'].to_pandas()\n",
    "\n",
    "sample = train.sample(2)\n",
    "\n",
    "for _, row in sample.iterrows():\n",
    "    print(\"Query:\")\n",
    "    print(row[\"query\"])\n",
    "    print()\n",
    "    print(\"Context:\")\n",
    "    print(row[\"context\"])\n",
    "    print()\n",
    "    print(\"Response:\")\n",
    "    print(row[\"response\"])\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c426568f",
   "metadata": {},
   "source": [
    "In hindsight, I should have provided GPT-5 with clearer instructions for response formulation, but the results are sufficient.\n",
    "\n",
    "**Now, let’s fine-tune TinyLlama. This training script assumes access to GPU acceleration. If you do not have a GPU, the script will fail. When using a CPU, you should adapt this code to a parameter-efficient fine-tuning approach, such as LoRA.**\n",
    "\n",
    "First, we prepare the training dataset using `RagDatasetBuilder`. This object is hardcoded for this specific task. It takes a pandas DataFrame with the columns `context`, `query`, and `response`, and converts them into a `datasets.Dataset` suitable for RAG fine-tuning.\n",
    "\n",
    "When fine-tuning, it is important to mask the query and context and train only on the response. Failing to do so slows convergence and can encourage the model to memorize the question and context rather than learn to generate good answers. `RagDatasetBuilder` handles this masking automatically, and the `sanity_check` method allows you to inspect exactly what is happening under the hood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dee3974c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb23f5106352429daec087ee0708b9b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "FULL MODEL INPUT (conditioning + target)\n",
      "================================================================================\n",
      "<|system|>\n",
      "You answer questions strictly using the provided context.\n",
      "\n",
      "<|user|>\n",
      "Context: kamtschatkana.  **Fifty individually caged abalone were held in a common aquarium tank with a constant flow of fresh ambient seawater and fed ad libitum on kelp (Nereocystis leutkeuna). ** The abalone were divided into five groups of ten animals each. Every group had a similar mean weight (78 g) and length (7 cm).\n",
      "Question: What is the scientific name for the common abalone?\n",
      "<|assistant|> The context mentions *kamtschatkana*, indicating the scientific name referenced there.</s>\n",
      "\n",
      "================================================================================\n",
      "TOKENS CONTRIBUTING TO LOSS (response only)\n",
      "================================================================================\n",
      "The context mentions *kamtschatkana*, indicating the scientific name referenced there.</s>\n",
      "\n",
      "================================================================================\n",
      "MASKING STATS\n",
      "================================================================================\n",
      "Total tokens     : 153\n",
      "Masked tokens    : 134\n",
      "Unmasked tokens  : 19\n",
      "================================================================================\n",
      "Sanity check passed: response-only masking is correct.\n"
     ]
    }
   ],
   "source": [
    "from modules import RagDatasetBuilder\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "builder = RagDatasetBuilder(tokenizer)\n",
    "dataset = builder.build_dataset(train)\n",
    "\n",
    "builder.sanity_check(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c5ed19",
   "metadata": {},
   "source": [
    "Now that the dataset is ready to go, lets put together the rest of the training script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c774c99",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "222b66da3ebe4503ba1815966039ebe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/101 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model is already on multiple devices. Skipping the move to device specified in `args`.\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 2}.\n",
      "/Users/jwkle/Documents/DDS/rag/.conda/lib/python3.10/site-packages/torch/utils/data/dataloader.py:692: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='21' max='21' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [21/21 01:25, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>2.562500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1.496600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>1.044600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>0.447200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./tinyllama-finetuned/tokenizer_config.json',\n",
       " './tinyllama-finetuned/special_tokens_map.json',\n",
       " './tinyllama-finetuned/chat_template.jinja',\n",
       " './tinyllama-finetuned/tokenizer.json')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, TrainingArguments\n",
    "from trl import SFTTrainer\n",
    "import torch\n",
    "\n",
    "model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./tinyllama-finetuned\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=8,\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-4,\n",
    "    bf16=True,\n",
    "    fp16=False,\n",
    "    logging_steps=5,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    args=training_args,\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "trainer.model.save_pretrained(\"./tinyllama-finetuned\")\n",
    "tokenizer.save_pretrained(\"./tinyllama-finetuned\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af02e9fc",
   "metadata": {},
   "source": [
    "Now, let's see how the quality of responses has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cca45908",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from modules import RAG\n",
    "\n",
    "generator = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"./tinyllama-finetuned\",\n",
    "    max_new_tokens=150,\n",
    "    temperature=0.1\n",
    "    )\n",
    "\n",
    "rag = RAG(retriever, generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec3a4b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "What sort of sauce goes well with abalone?\n",
      "\n",
      "Response:\n",
      "The context notes soy sauce and other seasonings go well with abalone boiled with soy sauce.\n",
      "\n",
      "Context:\n",
      "**There are two types of cooking methods for Ni-awabi (boiled abalones): Saka-ni (sake-boiling), in which abalones are boiled with a lot of sake, and shoyu-ni (soy sauce-boiling), in which abalones are boiled with soy sauce and other seasonings.**\n",
      "\n",
      "Source:\n",
      "https://sushiuniversity.jp/visual-dictionary/?Name=Japanese-abalone-(kuro-awabi)\n"
     ]
    }
   ],
   "source": [
    "query = \"What sort of sauce goes well with abalone?\"\n",
    "\n",
    "result = rag.answer_query(query)\n",
    "\n",
    "print(\"Query:\")\n",
    "print(query)\n",
    "print()\n",
    "print(\"Response:\")\n",
    "print(result[\"response\"])\n",
    "print()\n",
    "print(\"Context:\")\n",
    "print(result[\"context\"])\n",
    "print()\n",
    "print(\"Source:\")\n",
    "print(result[\"url\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1b26c",
   "metadata": {},
   "source": [
    "Final step, post your model to HuggingFace. This is reccomended if you intend in deploying it to HuggingFace Spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bee330c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911175b8819e4b71b4d0e5c7a19c9b52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing Files (0 / 0): |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "305bbc8b553a44eeb97be91286626a41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New Data Upload: |          |  0.00B /  0.00B            "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n",
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/LoneWolfgang/tinyllama-for-abalone-RAG/commit/e161dd87667ea2bf6470f2c1587f97f997119c60', commit_message='Upload tokenizer', commit_description='', oid='e161dd87667ea2bf6470f2c1587f97f997119c60', pr_url=None, repo_url=RepoUrl('https://huggingface.co/LoneWolfgang/tinyllama-for-abalone-RAG', endpoint='https://huggingface.co', repo_type='model', repo_id='LoneWolfgang/tinyllama-for-abalone-RAG'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.model.push_to_hub(\"LoneWolfgang/tinyllama-for-abalone-RAG\")\n",
    "trainer.processing_class.push_to_hub(\"LoneWolfgang/tinyllama-for-abalone-RAG\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".conda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
